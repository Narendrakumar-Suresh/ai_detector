# -*- coding: utf-8 -*-
"""AI_detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TJTLeckO0gsHqv2u1wjYSzmVx9ZuZt0T
"""

from google.colab import files
files.upload()

# %pip install kagglehub

import kagglehub

# Download latest version
path = kagglehub.dataset_download("sunilthite/llm-detect-ai-generated-text-dataset")

print("Path to dataset files:", path)

import nltk
nltk.download('punkt_tab')

# %pip install nltk

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#hyperparameters
num_epochs=30
batch_size=64
learning_rate=5e-4

df=pd.read_csv('/root/.cache/kagglehub/datasets/sunilthite/llm-detect-ai-generated-text-dataset/versions/1/Training_Essay_Data.csv')
df.head()

train_text, val_text, train_labels, val_labels = train_test_split(
    df['text'], df['generated'], test_size=0.3, train_size=0.7, random_state=42
)

from torch.nn.utils.rnn import pad_sequence
from collections import Counter
from nltk.tokenize import word_tokenize

def tokenize(text):
    return word_tokenize(text.lower())

tokenized_texts = [tokenize(t) for t in df['text'].astype(str)]
all_tokens = [token for tokens in tokenized_texts for token in tokens]
vocab = {token: idx + 2 for idx, (token, _) in enumerate(Counter(all_tokens).most_common())}
vocab['<PAD>'] = 0
vocab['<UNK>'] = 1

def encode(tokens):
    return [vocab.get(token, vocab['<UNK>']) for token in tokens]

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = list(texts)  # list() instead of tolist() to avoid pandas overhead
        self.labels = list(labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        tokens = tokenize(self.texts[idx])
        ids = torch.tensor(encode(tokens), dtype=torch.long)
        label = torch.tensor(self.labels[idx], dtype=torch.float32)
        return ids, label

# Collate function for dynamic padding
def collate_fn(batch):
    texts, labels = zip(*batch)
    texts = pad_sequence(texts, batch_first=True, padding_value=vocab['<PAD>'])
    labels = torch.tensor(labels, dtype=torch.float32)
    return texts, labels

train_dataset = TextDataset(train_text, train_labels)
val_dataset = TextDataset(val_text, val_labels)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
validation_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)

class LSTM(nn.Module):
  def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, num_layers=3, dropout=0.3):
    super(LSTM,self).__init__()
    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)
    self.fc1 = nn.Linear(hidden_dim, 128)
    self.fc2 = nn.Linear(128, 64)
    self.fc3 = nn.Linear(64, 1)
    self.relu = nn.ReLU()

  def forward(self, x):
        embedded = self.embedding(x)
        _, (hidden, _) = self.lstm(embedded)
        x = hidden[-1]  # shape: [batch_size, hidden_dim]
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x.squeeze(1)

vocab_size = len(vocab)
print("Declared vocab_size:", vocab_size)
print("Max index in vocab:", max(vocab.values()))
vocab_size=max(vocab.values())+1
print("Updated vocab_size:", vocab_size)

for x, y in train_loader:
    print("Max token index in batch:", x.max().item())
    break

model = LSTM(vocab_size).to(device)
criterion=nn.BCEWithLogitsLoss()
optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)

"""### Training"""

loss_tracker=[]
n_steps=len(train_loader)

for i in range(10):
  model.train()
  running_loss=0.00

  for j, (text,label) in enumerate(train_loader):
    text = text.to(device).long()
    label = label.to(device).float()

    output=model(text)
    loss=criterion(output,label)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    running_loss+=loss.item()
    if (j + 1) % 400 == 0:
            print(f'Epoch [{i+1}/{num_epochs}], Step [{j+1}/{n_steps}], Loss: {loss.item():.4f}')

  avg_loss = running_loss / len(train_loader)
  loss_tracker.append(avg_loss)

plt.plot(loss_tracker)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epoch')
plt.show()

import torch.nn.functional as F

model.eval()  # Set the model to evaluation mode

with torch.no_grad():
    n_correct = 0
    n_samples = 0

    for j, (text, label) in enumerate(validation_loader):
        text = text.to(device)
        label = label.to(device)

        output = model(text)

        prob = torch.sigmoid(output)

        predicted = (prob > 0.5).float()

        n_samples += label.size(0)
        n_correct += (predicted.squeeze() == label).sum().item()

    acc = 100.0 * n_correct / n_samples
    print(f'Accuracy: {acc:.2f}%')