# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nt5z9fY8PRRaRC-ceJqA8IQ9_saTmO14
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install kagglehub
# %pip install nltk

from google.colab import files
files.upload()

import kagglehub

# Download latest version
path = kagglehub.dataset_download("sunilthite/llm-detect-ai-generated-text-dataset")

print("Path to dataset files:", path)

import nltk
nltk.download('punkt_tab')

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#hyperparameters
num_epochs=10
batch_size=64
learning_rate=5e-4

df=pd.read_csv(f'{path}/Training_Essay_Data.csv')
df.head()

train_text, val_text, train_labels, val_labels = train_test_split(
    df['text'], df['generated'], test_size=0.3, train_size=0.7, random_state=42
)

"""## Data preperation"""

from torch.nn.utils.rnn import pad_sequence
from collections import Counter
from nltk.tokenize import word_tokenize

def tokenize(text):
    return word_tokenize(text.lower())

tokenized_texts = [tokenize(t) for t in df['text'].astype(str)]
all_tokens = [token for tokens in tokenized_texts for token in tokens]
vocab = {token: idx + 2 for idx, (token, _) in enumerate(Counter(all_tokens).most_common())}
vocab['<PAD>'] = 0
vocab['<UNK>'] = 1

def encode(tokens):
    return [vocab.get(token, vocab['<UNK>']) for token in tokens]

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = list(texts)  # list() instead of tolist() to avoid pandas overhead
        self.labels = list(labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        tokens = tokenize(self.texts[idx])
        ids = torch.tensor(encode(tokens), dtype=torch.long)
        label = torch.tensor(self.labels[idx], dtype=torch.float32)
        return ids, label

# Collate function for dynamic padding
def collate_fn(batch):
    texts, labels = zip(*batch)
    texts = pad_sequence(texts, batch_first=True, padding_value=vocab['<PAD>'])
    labels = torch.tensor(labels, dtype=torch.float32)
    return texts, labels

train_dataset = TextDataset(train_text, train_labels)
val_dataset = TextDataset(val_text, val_labels)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
validation_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)

"""## Model"""

class LSTM(nn.Module):
  def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, num_layers=3, dropout=0.3):
    super(LSTM,self).__init__()
    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)
    self.fc1 = nn.Linear(hidden_dim, 128)
    self.fc2 = nn.Linear(128, 64)
    self.fc3 = nn.Linear(64, 1)
    self.relu = nn.ReLU()

  def forward(self, x):
        embedded = self.embedding(x)
        _, (hidden, _) = self.lstm(embedded)
        x = hidden[-1]  # shape: [batch_size, hidden_dim]
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x.squeeze(1)

vocab_size = len(vocab)
print("Declared vocab_size:", vocab_size)
print("Max index in vocab:", max(vocab.values()))
vocab_size=max(vocab.values())+1
print("Updated vocab_size:", vocab_size)

for x, y in train_loader:
    print("Max token index in batch:", x.max().item())
    break

model = LSTM(vocab_size).to(device)
criterion=nn.BCEWithLogitsLoss()
optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)

"""## Training"""

import time

loss_tracker = []
log_tracker = []  # To keep logs if needed for later
n_steps = len(train_loader)
total_start_time = time.time()

for epoch in range(10):
    model.train()
    running_loss = 0.0
    epoch_start_time = time.time()

    for step, (text, label) in enumerate(train_loader):
        text = text.to(device).long()
        label = label.to(device).float()

        output = model(text)
        loss = criterion(output, label)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        log = f"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{n_steps}], Loss: {loss.item():.4f}"
        print(log)
        log_tracker.append(log)

    avg_loss = running_loss / len(train_loader)
    loss_tracker.append(avg_loss)

    epoch_time = time.time() - epoch_start_time
    print(f"Epoch [{epoch+1}] completed in {epoch_time:.2f} seconds. Avg Loss: {avg_loss:.4f}")

total_time = time.time() - total_start_time
print(f"\nTraining completed in {total_time:.2f} seconds.")

"""## Epoch vs Loss"""

plt.plot(loss_tracker)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epoch')
plt.show()

"""## Evaluvation"""

import torch.nn.functional as F

model.eval()  # Set the model to evaluation mode

with torch.no_grad():
    n_correct = 0
    n_samples = 0

    for j, (text, label) in enumerate(validation_loader):
        text = text.to(device)
        label = label.to(device)

        output = model(text)

        prob = torch.sigmoid(output)

        predicted = (prob > 0.5).float()

        n_samples += label.size(0)
        n_correct += (predicted.squeeze() == label).sum().item()

    acc = 100.0 * n_correct / n_samples
    print(f'Accuracy: {acc:.2f}%')

"""## Saving model"""

FILE="model.pth"
torch.save(model.state_dict(),FILE)

import pickle

# Save the vocab to a file
with open("vocab.pkl", "wb") as f:
    pickle.dump(vocab, f)

"""## Using model"""

# device='cpu'
# loaded_model=LSTM(vocab_size)
# loaded_model.load_state_dict(torch.load(FILE,map_location=device))
# loaded_model.eval()