# AI-Generated Text Detector

This application uses a pre-trained LSTM model to detect whether a given text is AI-generated or human-written. You can either paste text or upload a `.txt` file to test the model. The model evaluates the probability that the text was generated by AI.

## Features

* **Text Input:** Paste text directly into the text area to check its authenticity.
* **File Upload:** Upload a `.txt` file containing the text you want to analyze.
* **Prediction:** The app returns the likelihood (in percentage) that the text is AI-generated.

## How It Works

1. The app uses an LSTM-based model trained to classify text as either human-written or AI-generated.
2. The model processes the text by tokenizing it, encoding the tokens into numerical indices, and passing them through the LSTM network.
3. The output is a probability score, which represents the likelihood that the input text is AI-generated.

## Setup

1. Clone the repository to your local machine:

   ```bash
   git clone https://github.com/yourusername/ai-generated-text-detector.git
   cd ai-generated-text-detector
   ```

2. Create a virtual environment:

   ```bash
   python -m venv .env
   ```

3. Activate the virtual environment:

   * On Windows:

     ```bash
     .env\Scripts\activate
     ```
   * On macOS/Linux:

     ```bash
     source .env/bin/activate
     ```

4. Install the required dependencies:

   ```bash
   pip install -r requirements.txt
   ```

5. Download the necessary NLTK resources:

   ```python
   import nltk
   nltk.download('punkt')
   ```

6. Run the app:

   ```bash
   streamlit run main.py
   ```

7. Open your browser and go to `http://localhost:8501` to use the app.

## Files & Structure

* `main.py`: The main script that runs the Streamlit app.
* `vocab.pkl`: The vocabulary file used for encoding text tokens.
* `model.pth`: The pre-trained model weights for the LSTM.
* `requirements.txt`: The list of Python dependencies for the project.

## Dependencies

* Streamlit: For building the web interface.
* PyTorch: For the machine learning model.
* NLTK: For tokenizing text.
* Pickle: For loading the pre-trained model and vocabulary.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.